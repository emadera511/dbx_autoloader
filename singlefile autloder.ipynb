{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databrics Autoloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql.functions import col, size, split, input_file_name \n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, DateType, TimestampType, IntergerType \n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will create a function that will have the autloader logic to use.   This code will be reuse to load different files into the unity catalog using autoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoloader(\n",
    "        datasource, \n",
    "        format, \n",
    "        checkpoint,\n",
    "        table_name, \n",
    "        schema, \n",
    "        file\n",
    "):\n",
    "    logger.info(\"start reading data using readstream\")\n",
    "\n",
    "    query = (\n",
    "        spark.readstream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", format)\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint) # this is where autoloader will house metadata \n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"true\") # Includes existing files in the file path \n",
    "        .option(\"cloudFiles.useIncrementalListing\", \"true\") # use incremental listing to detect new files \n",
    "        .option(\"recursiveFileLookup\", \"true\") # enable recursive file lookup \n",
    "        .option(\"header\", \"true\")\n",
    "        .opiton(\"pathGlobFilter\", f\"*{file}*\") # this is a file patter that will be use to filter file name\n",
    "        .schema(schema)\n",
    "        .load(datasource)\n",
    "    )\n",
    "\n",
    "    # Running transofmation on streamed data\n",
    "\n",
    "    query = query.withColumn(\"input_file_name\", input_file_name())\n",
    "\n",
    "    query = query.withColumn(\"split\", split(query[\"input_file_name\"]), \"/\")\n",
    "\n",
    "    query = query.withColumn(\"input_file_name\", query[\"split\"][6])\n",
    "\n",
    "    query = query.drop(query[\"split\"])\n",
    "\n",
    "    load = (\n",
    "        query.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .tirgger(avialable=True)\n",
    "        .toTable(table_name)\n",
    "    )\n",
    "\n",
    "    return load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file using the above autoloader function \n",
    "\n",
    "For file we are going to autoload it contains 5 columns \"FRIST_NAME\", \"LAST_NAME\", \"USERID\", \"USER_EMAIL\", \"PHONE_NUMER\".  The file is pipe delimited therefore I have already use a delimiter in autloader code that will scan for pipe delimited.  \n",
    "\n",
    "Let set the schema for the file we are going to autoload.   See autoloader fucntion on how the schema is set while the data is readStream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"FIRST_NAME\", StringType(), True),\n",
    "        StructField(\"LAST_NAME\", StringType(), True), \n",
    "        StructField(\"USERID\", StringType(), True), \n",
    "        StructField(\"USER_EMAIL\", StringType(), True), \n",
    "        StructField(\"PHONE_NUMBER\", IntergerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are loading the data into the unity catalog of databricks we will use the following for catalog name \"em_catalog\", for the schema name we will use \"em_schema\".  we our file which is the file location with a file name em_user.  our location is in a databricks volume for exammple /volumes/em_catalog/em_schema/files.  \n",
    "\n",
    "for our checkpoint location we created a file location as such /volumes/em_catalog/em_schema/checkpoint inside of this file we create the checkpoint for the em_user checkpoint location.   \n",
    "\n",
    "When running the autoloader for the first time it will scan for directory for file name in the directory it will load all the files present in that directory.  If a new file lands on that directory the autoloader will only load the new data.  Why is this because of the checkpoint the checkpoint is a metadata that keeps track of the files that have been loaded.  Therefore only new files will loaded after running the autoloaded.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog =\"em_catalog\" \n",
    "schema_name = \"em_schema\"\n",
    "file = \"em_user\"\n",
    "table_name = f\"{catalog}.{schema_name}.{file}_inbound\" \n",
    "\n",
    "checkpoint = f\"/volumes/em_catalog/em_schema/checkpoint/{file}\"\n",
    "data_source = \"/volumes/em_catalog/em_schema/files\"\n",
    "\n",
    "load = autoloader(data_source, \"csv\", table_name, checkpoint, schema, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
