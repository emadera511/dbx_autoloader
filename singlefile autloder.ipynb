{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databrics Autoloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql.functions import col, size, split, input_file_name \n",
    "from pyspark.sql.types import StructType, StructField, StringType, DecimalType, DateType, TimestampType \n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will create a function that will have the autloader logic to use.   This code will be reuse to load different files into the unity catalog using autoloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoloader(\n",
    "        datasource, \n",
    "        format, \n",
    "        checkpoint,\n",
    "        table, \n",
    "        schema, \n",
    "        file\n",
    "):\n",
    "    logger.info(\"start reading data using readstream\")\n",
    "\n",
    "    query = (\n",
    "        spark.readstream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", format)\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint) # this is where autoloader will house metadata \n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"true\") # Includes existing files in the file path \n",
    "        .option(\"cloudFiles.useIncrementalListing\", \"true\") # use incremental listing to detect new files \n",
    "        .option(\"recursiveFileLookup\", \"true\") # enable recursive file lookup \n",
    "        .option(\"header\", \"true\")\n",
    "        .opiton(\"pathGlobFilter\", f\"*{file}*\") # this is a file patter that will be use to filter file name\n",
    "        .schema(schema)\n",
    "        .load(datasource)\n",
    "    )\n",
    "\n",
    "    # Running transofmation on streamed data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
